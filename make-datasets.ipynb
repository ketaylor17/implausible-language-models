{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c431696a48416f840fd958ffd7686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 11:31:11 INFO: Downloaded file to /Users/KateTaylor/stanza_resources/resources.json\n",
      "2024-12-07 11:31:11 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-12-07 11:31:12 INFO: File exists: /Users/KateTaylor/stanza_resources/en/default.zip\n",
      "2024-12-07 11:31:15 INFO: Finished downloading models and saved to /Users/KateTaylor/stanza_resources\n",
      "2024-12-07 11:31:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ff96332bc045859cf405e5b5cfe0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 11:31:15 INFO: Downloaded file to /Users/KateTaylor/stanza_resources/resources.json\n",
      "2024-12-07 11:31:15 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-12-07 11:31:15 INFO: Using device: cpu\n",
      "2024-12-07 11:31:15 INFO: Loading: tokenize\n",
      "2024-12-07 11:31:15 INFO: Loading: mwt\n",
      "2024-12-07 11:31:15 INFO: Loading: pos\n",
      "2024-12-07 11:31:16 INFO: Loading: lemma\n",
      "2024-12-07 11:31:16 INFO: Loading: depparse\n",
      "2024-12-07 11:31:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# import dataset and load stanza pipeline\n",
    "\n",
    "import data_processing_utils\n",
    "\n",
    "#!pip install stanza\n",
    "#!pip install datasets\n",
    "import stanza\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "train_ds = load_dataset(\"vesteinn/babylm\", split=\"train\")\n",
    "test_ds = load_dataset(\"vesteinn/babylm\", split=\"test\")\n",
    "val_ds = load_dataset(\"vesteinn/babylm\", split=\"validation\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# sample 100,000 sentences from each dataset\n",
    "#sampled_train_ds = train_ds.shuffle(seed=42).select(range(1000000))\n",
    "sampled_test_ds = test_ds.shuffle(seed=42).select(range(1000000))\n",
    "#sampled_val_ds = val_ds.shuffle(seed=42).select(range(1000000))\n",
    "\n",
    "#datasets={\"train\":sampled_train_ds}#,\"test\":sampled_test_ds,\"validation\":sampled_val_ds}\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')#,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded val dataset, 1000000 docs\n",
      "generated 1000000 stanza objects\n",
      "converted into 1098102 sentences\n",
      "saved to json file\n",
      "\n",
      "loaded test dataset, 1000000 docs\n",
      "generated 1000000 stanza objects\n",
      "converted into 1119330 sentences\n",
      "saved to json file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# validate\n",
    "sampled_val_ds = val_ds.shuffle(seed=42).select(range(1000000))\n",
    "\n",
    "# convert ds from list of strings -> list of list of python dicts\n",
    "docs = [ds_sentence['text'] for ds_sentence in sampled_val_ds]\n",
    "print('loaded val dataset,', len(docs), 'docs')\n",
    "in_docs = [stanza.Document([], text=d) for d in docs]\n",
    "out_docs = nlp(in_docs)\n",
    "print('generated', len(out_docs), 'stanza objects')\n",
    "out_arrs = data_processing_utils.convert_list_of_stanza_docs_into_list_of_list_of_dicts(out_docs)\n",
    "print('converted into', len(out_arrs), 'sentences')\n",
    "data_processing_utils.save_to_json_file(out_arrs, 'validation_data_1000000')\n",
    "print('saved to json file\\n')\n",
    "# test\n",
    "\n",
    "# convert ds from list of strings -> list of list of python dicts\n",
    "docs = [ds_sentence['text'] for ds_sentence in sampled_test_ds]\n",
    "print('loaded test dataset,', len(docs), 'docs')\n",
    "in_docs = [stanza.Document([], text=d) for d in docs]\n",
    "out_docs = nlp(in_docs)\n",
    "print('generated', len(out_docs), 'stanza objects')\n",
    "out_arrs = data_processing_utils.convert_list_of_stanza_docs_into_list_of_list_of_dicts(out_docs)\n",
    "print('converted into', len(out_arrs), 'sentences')\n",
    "data_processing_utils.save_to_json_file(out_arrs, 'testing_data_1000000')\n",
    "print('saved to json file\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded test dataset, 100000 docs\n",
      "generated 100000 stanza objects\n",
      "converted into 111857 sentences\n",
      "saved to json file\n",
      "\n",
      "loaded val dataset, 100000 docs\n",
      "generated 100000 stanza objects\n",
      "converted into 109780 sentences\n",
      "saved to json file\n",
      "\n",
      "loaded train dataset, 1000000 docs\n",
      "generated 1000000 stanza objects\n",
      "converted into 1095733 sentences\n",
      "saved to json file\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# convert ds from list of strings -> list of list of python dicts\n",
    "docs = [ds_sentence['text'] for ds_sentence in sampled_test_ds]\n",
    "print('loaded test dataset,', len(docs), 'docs')\n",
    "in_docs = [stanza.Document([], text=d) for d in docs]\n",
    "out_docs = nlp(in_docs)\n",
    "print('generated', len(out_docs), 'stanza objects')\n",
    "out_arrs = data_processing_utils.convert_list_of_stanza_docs_into_list_of_list_of_dicts(out_docs)\n",
    "print('converted into', len(out_arrs), 'sentences')\n",
    "data_processing_utils.save_to_json_file(out_arrs, 'testing_data_100000')\n",
    "print('saved to json file\\n')\n",
    "\n",
    "# validate\n",
    "# convert ds from list of strings -> list of list of python dicts\n",
    "docs = [ds_sentence['text'] for ds_sentence in sampled_val_ds]\n",
    "print('loaded val dataset,', len(docs), 'docs')\n",
    "in_docs = [stanza.Document([], text=d) for d in docs]\n",
    "out_docs = nlp(in_docs)\n",
    "print('generated', len(out_docs), 'stanza objects')\n",
    "out_arrs = data_processing_utils.convert_list_of_stanza_docs_into_list_of_list_of_dicts(out_docs)\n",
    "print('converted into', len(out_arrs), 'sentences')\n",
    "data_processing_utils.save_to_json_file(out_arrs, 'validation_data_100000')\n",
    "print('saved to json file\\n')\n",
    "\n",
    "# train (big)\n",
    "# convert ds from list of strings -> list of list of python dicts\n",
    "docs = [ds_sentence['text'] for ds_sentence in sampled_train_ds]\n",
    "print('loaded train dataset,', len(docs), 'docs')\n",
    "in_docs = [stanza.Document([], text=d) for d in docs]\n",
    "out_docs = nlp(in_docs)\n",
    "print('generated', len(out_docs), 'stanza objects')\n",
    "out_arrs = data_processing_utils.convert_list_of_stanza_docs_into_list_of_list_of_dicts(out_docs)\n",
    "print('converted into', len(out_arrs), 'sentences')\n",
    "data_processing_utils.save_to_json_file(out_arrs, 'training_data_1000000')\n",
    "print('saved to json file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109780\n"
     ]
    }
   ],
   "source": [
    "out_arrs = data_processing_utils.load_from_json_file('validation_data_100000')\n",
    "print(len(out_arrs))\n",
    "# use list of list of dicts that was just generated to generate the modified datasets\n",
    "# this can also be done by loading the json file\n",
    "data_as_original_sentences = data_processing_utils.get_original_strings(out_arrs)\n",
    "data_processing_utils.save_to_json_file(data_as_original_sentences, 'validation_data_100000_as_original_sentences')\n",
    "\n",
    "data_as_reversed_sentences = data_processing_utils.get_reversed_strings(out_arrs)\n",
    "data_processing_utils.save_to_json_file(data_as_reversed_sentences, 'validation_data_100000_as_reversed_sentences')\n",
    "\n",
    "data_as_head_initial_sentences = data_processing_utils.get_head_initial_strings(out_arrs)\n",
    "data_processing_utils.save_to_json_file(data_as_head_initial_sentences, 'validation_data_100000_as_head_initial_sentences')\n",
    "\n",
    "data_as_head_final_sentences = data_processing_utils.get_head_final_strings(out_arrs)\n",
    "data_processing_utils.save_to_json_file(data_as_head_final_sentences, 'validation_data_100000_as_head_final_sentences')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantics_code",
   "language": "python",
   "name": "semantics_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
